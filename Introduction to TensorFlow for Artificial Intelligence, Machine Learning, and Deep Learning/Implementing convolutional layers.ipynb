{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9fae2fc",
   "metadata": {},
   "source": [
    "# Implementing convolutional layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fdefbd",
   "metadata": {},
   "source": [
    "Certainly! Here's an overview of some common use cases for each of the layers you mentioned, as well as additional layers commonly used in deep learning models:\n",
    "\n",
    "1. Conv2D: Convolutional layers are widely used in computer vision tasks, such as image classification, object detection, and image segmentation. They are designed to extract meaningful features from images by applying convolutional filters.\n",
    "\n",
    "2. MaxPooling2D: MaxPooling2D layers are often used in conjunction with Conv2D layers. They downsample the feature maps by selecting the maximum value within each local region. MaxPooling2D helps reduce spatial dimensions, capture important features at different scales, and improve model performance.\n",
    "\n",
    "3. Flatten: The Flatten layer is frequently used to transition from convolutional layers to fully connected layers. It reshapes the multidimensional output from the previous layers into a one-dimensional vector, which is then fed into dense layers or output layers.\n",
    "\n",
    "4. Dense: Dense layers, also known as fully connected layers, are used in various tasks, including image classification, natural language processing, and regression. Each neuron in a Dense layer is connected to every neuron from the previous layer. Dense layers allow the model to learn complex relationships between the input and output, and their activation functions can be tailored to the specific task.\n",
    "\n",
    "5. Dropout: Dropout layers are used to prevent overfitting in deep learning models. They randomly \"drop out\" a fraction of the neurons during training, which helps in reducing interdependent learning among neurons and improving the generalization ability of the model.\n",
    "\n",
    "6. Activation: Activation layers apply non-linear transformations to the output of the previous layer. Common activation functions include ReLU (Rectified Linear Unit), sigmoid, and softmax. Activation layers introduce non-linearity into the model, enabling it to learn more complex patterns and make predictions.\n",
    "\n",
    "7. BatchNormalization: Batch Normalization layers normalize the activations of the previous layer, reducing the internal covariate shift. They help stabilize and accelerate the training process, improve gradient flow, and regularize the model.\n",
    "\n",
    "8. Recurrent layers (e.g., LSTM, GRU): Recurrent layers are commonly used in natural language processing tasks and time series analysis. LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) layers are variants of recurrent layers that can capture sequential dependencies and long-term dependencies in the data.\n",
    "\n",
    "9. Embedding: Embedding layers are often used in natural language processing tasks to convert categorical data, such as words or characters, into dense vectors. Embeddings capture semantic relationships between words, enabling the model to learn representations that encode useful information.\n",
    "\n",
    "These are just a few examples of the layers commonly used in deep learning models. The choice and arrangement of layers depend on the specific problem, the nature of the data, and the architecture of the model. Experimentation and understanding the requirements of the task are essential in selecting the appropriate layers for a given use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08121162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f35a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= keras.models.Sequential([\n",
    "    Flatten(),  #This layer is used to flatten the input data,\n",
    "    \n",
    "    Dense(128 , activation=tf.nn.relu),# hidden layer is a fully connected layer with 128 units. \n",
    "    \n",
    "    Dense(10 , activation=tf.nn.softmax)  # output probabilities for each class.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168d1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83769032",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential([\n",
    "    \n",
    "    #Here we're specifying the first convolution. We're asking keras to \n",
    "   # generate 64 filters for us. These filters are 3 by 3, their activation is relu, \n",
    "   # which means the negative values will be thrown way, and finally the input shape \n",
    "   # is as before, the 28 by 28. That extra 1 just means that we are tallying using a \n",
    "   # single byte for color\n",
    "   # depth. As we saw before our image is our gray scale, so we just use one byte.\n",
    "    \n",
    "    Conv2D( 64   ,(3,3) , activation='relu' , input_shape=(28,28,1)        ),\n",
    "    \n",
    "    MaxPool2D(2,2),\n",
    "    \n",
    "     Conv2D(64,(3,3) , activation=\"relu\"),\n",
    "    MaxPool2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(128 , activation='relu'),\n",
    "    Dense(10 , activation='softmax'),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79636f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
