{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caebe738",
   "metadata": {},
   "source": [
    "# Implementing pooling layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff18ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceca396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17271d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential([\n",
    "        #Here we're specifying the first convolution. We're asking keras to \n",
    "   # generate 64 filters for us. These filters are 3 by 3, their activation is relu, \n",
    "   # which means the negative values will be thrown way, and finally the input shape \n",
    "   # is as before, the 28 by 28. That extra 1 just means that we are tallying using a \n",
    "   # single byte for color\n",
    "   # depth. As we saw before our image is our gray scale, so we just use one byte.\n",
    "    \n",
    "    Conv2D( 64   ,(3,3) , activation='relu' , input_shape=(28,28,1)        ),\n",
    "    \n",
    "    MaxPool2D(2,2),  # it's a two-by-two pool, so for every four pixels, the biggest one will survive \n",
    "    \n",
    "    \n",
    "    \n",
    "    #and another max-pooling layer so that the network can learn another set of \n",
    "    #convolutions on top of the existing one, and then again, pool to reduce the size. \n",
    "     Conv2D(64,(3,3) , activation=\"relu\"),\n",
    "    MaxPool2D(2,2),\n",
    "    \n",
    "    \n",
    "    \n",
    "    Flatten(),  #This layer is used to flatten the input data,\n",
    "    \n",
    "    Dense(128 , activation=tf.nn.relu),# hidden layer is a fully connected layer with 128 units. \n",
    "    \n",
    "    Dense(10 , activation=tf.nn.softmax)  # output probabilities for each class.\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f1669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debac277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 64)        640       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 13, 13, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               204928    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#This allows you to inspect the layers of the model, \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a91df15",
   "metadata": {},
   "source": [
    "###################################  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f44bde",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e6596",
   "metadata": {},
   "source": [
    "conv2d_2 (Conv2D)\n",
    "\n",
    "means that you can't use a one pixel margin all around the image, so the output of the convolution will be two pixels smaller on x, and two pixels smaller on y. So, that's y with a three by three filter, our output from the 28 by 28 image, is now 26 by 26, we've removed that one pixel on x and y, and each of the borders. So, next is the first of the max-pooling layers. Now, remember we specified it to be two-by-two, thus turning four pixels into one, and having our x and y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79824c73",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038d8c5",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b1628",
   "metadata": {},
   "source": [
    " max_pooling2d_2 (MaxPooling2D)  \n",
    "    \n",
    "    \n",
    "first of the max-pooling layers. \n",
    ", we specified it to be two-by-two\n",
    "thus turning four pixels into one, and having our x and y\n",
    "So, now our output gets reduced from 26 by 26, to 13 by 13. The convolutions will then operate on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d101fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df44195b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02a2983c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "169*4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a97c30c",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3a4862",
   "metadata": {},
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a7fc0a",
   "metadata": {},
   "source": [
    "conv2d_3 (Conv2D)\n",
    "\n",
    "of course, we lose the one pixel margin as before,\n",
    " so we're down to 11 by 11, add another two-by-two max-pooling to have this rounding down, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b8902",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ba59d",
   "metadata": {},
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac9e127",
   "metadata": {},
   "source": [
    "max_pooling2d_3 (MaxPooling2D)\n",
    "\n",
    "and went down, down to five-by-five images. \n",
    "So, now our dense neural network is the same as before, \n",
    "but it's being fed with five-by-five images instead of 28 by 28 ones.\n",
    " But remember, it's not just one compress five-by-five image instead of the original 28 by 28, there are a number of convolutions per image that we specified,\n",
    " in this case 64. So, there are 64 new images of five-by-five that had been fed in. Flatten that out and you have 25 pixels times 64, which is 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b264f1ac",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c518cac",
   "metadata": {},
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf6542",
   "metadata": {},
   "source": [
    " flatten_1 (Flatten)\n",
    "    \n",
    "which is 1600. So, you can see that the new flattened layer has 1,600 elements in it, as opposed to the 784 that you had previously. This number is impacted by the parameters that you set when defining the convolutional 2D layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac3799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f985ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c39bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad95031b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde41a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d51a144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b00f3b",
   "metadata": {},
   "source": [
    "###### ALL Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872c129",
   "metadata": {},
   "source": [
    "This next line of code will then create a pooling layer. It's max-pooling because we're going to take the maximum value. We're saying it's a two-by-two pool, so for every four pixels, the biggest one will survive as shown earlier. We then add another convolutional layer, and another max-pooling layer so that the network can learn another set of convolutions on top of the existing one, and then again, pool to reduce the size. So, by the time the image gets to the flatten to go into the dense layers, it's already much smaller. It's being quartered, and then quartered again. So, its content has been greatly simplified, the goal being that the convolutions will filter it to the features that determine the output. A really useful method on the model is the model.summary method. This allows you to inspect the layers of the model, and see the journey of the image through the convolutions, and here is the output. It's a nice table showing us the layers, and some details about them including the output shape. It's important to keep an eye on the output shape column. When you first look at this, it can be a little bit confusing and feel like a bug. After all, isn't the data 28 by 28, so y is the output, 26 by 26. The key to this is remembering that the filter is a three by three filter. Consider what happens when you start scanning through an image starting on the top left. So, for example with this image of the dog on the right, you can see zoomed into the pixels at its top left corner. You can't calculate the filter for the pixel in the top left, because it doesn't have any neighbors above it or to its left. In a similar fashion, the next pixel to the right won't work either because it doesn't have any neighbors above it. So, logically, the first pixel that you can do calculations on is this one, because this one of course has all eight neighbors that a three by three filter needs. This when you think about it, means that you can't use a one pixel margin all around the image, so the output of the convolution will be two pixels smaller on x, and two pixels smaller on y. If your filter is five-by-five for similar reasons, your output will be four smaller on x, and four smaller on y. So, that's y with a three by three filter, our output from the 28 by 28 image, is now 26 by 26, we've removed that one pixel on x and y, and each of the borders. So, next is the first of the max-pooling layers. Now, remember we specified it to be two-by-two, thus turning four pixels into one, and having our x and y. So, now our output gets reduced from 26 by 26, to 13 by 13. The convolutions will then operate on that, and of course, we lose the one pixel margin as before, so we're down to 11 by 11, add another two-by-two max-pooling to have this rounding down, and went down, down to five-by-five images. So, now our dense neural network is the same as before, but it's being fed with five-by-five images instead of 28 by 28 ones. But remember, it's not just one compress five-by-five image instead of the original 28 by 28, there are a number of convolutions per image that we specified, in this case 64. So, there are 64 new images of five-by-five that had been fed in. Flatten that out and you have 25 pixels times 64, which is 1600. So, you can see that the new flattened layer has 1,600 elements in it, as opposed to the 784 that you had previously. This number is impacted by the parameters that you set when defining the convolutional 2D layers. Later when you experiment, you'll see what the impact of setting what other values for the number of convolutions will be, and in particular, you can see what happens when you're feeding less than 784 over all pixels in. Training should be faster, but is there a sweet spot where it's more accurate? Well, let's switch to the workbook, and we can try it out for ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28535a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
